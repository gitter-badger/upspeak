/**
 * global_id_seq is used to generate ids by generate_id() by default.
 */
create sequence public.global_id_seq
    start with 1
    increment by 1
    no minvalue
    no maxvalue
    cache 1
    cycle;

/**
 * generate_id creates unique IDs based on timestamp
 *
 * Generates 64-bit integer (bigint) id using timestamp and a given sequence. 46
 * bits for timestamp, 5 bits for partition id (default 1), followed by 13 bits
 * of sequence id. This is a modification of Instagram''s Postgresql based id
 * generation system.
 */
create function public.generate_id(
        seq_name text default 'public.global_id_seq'::text,
        partition_id integer default 1, out result bigint
    ) returns bigint
    language plpgsql parallel safe
    as $$declare
    our_epoch bigint := 1512066600000;
    seq_id bigint;
    now_millis bigint;
begin
    select nextval(seq_name) % 8192 into seq_id;

    select floor(extract(epoch from clock_timestamp()) * 1000) into now_millis;
    result := (now_millis - our_epoch) << 18;
    result := result | (partition_id << 13);
    result := result | (seq_id);
end;
$$;

/**
 * Create users table. All exposed API calls happen in some users' scope.
 */
create table public.users (
    id bigint default generate_id() not null primary key, -- autogenerated
    email_primary text not null unique, -- Main communication email
    display_name text, -- User's publicly displayed name
    password text not null, -- Please store a hash here
    created_at timestamp with time zone not null, -- Time since when user is a member
    is_verified boolean default false not null, -- Whether user account was verified
    is_active boolean default false not null -- Whether user account is active
);


/**
 * Teams info
 */
create table public.teams (
    id bigint default generate_id() not null primary key,
    primary_contact bigint not null references public.users (id),
    name text not null
);

/**
 * Link table for storing lists of team members
 */
create table public.team_members (
    team_id bigint not null references public.teams(id),
    user_id bigint not null references public.users(id),

    -- Combo primary key
    primary key (team_id, user_id)
);


/**
 * Namespaces info
 */
create table public.namespaces (
    slug character varying(256) not null primary key,
    team_id bigint not null references public.teams(id),
    user_id bigint not null references public.users(id)
);

-- Btree index of team IDs in namespaces to quickly lookup the team for a
-- specific slug
create index namespace_team_id_idx on public.namespaces using btree (team_id);

-- Btree index of team IDs in namespaces to quickly lookup the user for a
-- specific slug
create index namespace_user_id_idx on public.namespaces using btree (user_id);


/**
 * Workspaces info
 */
create table public.workspaces (
    id bigint default generate_id() not null primary key,
    namespace_slug character varying(256) not null references public.namespaces (slug),
    visibility_level character varying(10) -- Can be 'hidden', 'private', 'public'
);

/**
 * Link table for storing lists of workspace members
 */
create table public.workspace_members (
    workspace_id bigint not null references public.workspaces(id),
    user_id bigint not null references public.users(id),
    access_level character varying(10), -- Can be 'read', 'write' or 'admin'

    -- Combo primary key
    primary key (workspace_id, user_id)
);

/**
 * List of available node types
 */
create table public.node_data_types (
    id character varying(256) not null primary key,
    is_editable boolean default true,

    -- Optional attribute storage for node data type. This can be used to store
    -- data type specific information. This field is not indexed.
    attrs jsonb
);

-- create a `markdown` node type by default
insert into public.node_data_types
    values ('markdown', true, null);

/**
 * Create nodes table. Our central data structure
 */
create table nodes (
    id bigint default generate_id() not null primary key,
    author_id bigint not null references public.users (id),
    data_type character varying(256) not null references public.node_data_types(id),
    source_node_id bigint references public.nodes (id),
    created_at timestamp with time zone not null,
    updated_at timestamp with time zone not null,
    updated_by bigint references public.users(id),
    subject text,
    body text,
    rich_data jsonb,
    in_reply_to bigint references public.nodes(id),
    attrs jsonb,
    is_archived bool not null default false -- soft delete
);

-- Btree index of authors of nodes
create index node_author_idx on public.nodes using btree (author_id);

-- Btree index of source nodes to speed up thread lookups
create index node_source_node_idx on public.nodes using btree (source_node_id);

-- GIN indexing for node attrs
create index node_attrs_idx on public.nodes using gin (attrs jsonb_path_ops);

/**
 * Create node revisions table
 */
create schema if not exists audit;

create table audit.node_revisions (
    subject text,
    body text,
    rich_data jsonb,
    created_at timestamp with time zone default now() not null,
    committer_id bigint not null references public.users(id),
    node_id bigint not null references public.nodes(id),

    -- Combo primary key
    primary key (node_id, created_at)
);


-- Trigger function to keep node_revisions updated with node history

create function trigger_on_node_revision()
    returns trigger
    language plpgsql as $body$
begin
    if tg_op = 'UPDATE' then
        if old.subject <> new.subject or old.body <> new.body or old.rich_data <> new.rich_data then
            -- Create node revision only if node's content has changed
            if old.updated_at is null and old.updated_by is null then
                -- First edit of node
                insert into audit.node_revisions (node_id, subject, body, rich_data, created_at, committer_id)
                values (old.id, old.subject, old.body, old.rich_data, old.created_at, old.author_id);
            else
                -- Subsequent edits of node
                insert into audit.node_revisions (node_id, subject, body, rich_data, created_at, committer_id)
                values (old.id, old.subject, old.body, old.rich_data, old.updated_at, old.updated_by);
            end if;
        end if;
        return new;
    end if;

    if tg_op = 'DELETE' then
        delete from audit.node_revisions where node_id = old.id;
        return old;
    end if;
end; $body$;

create trigger trigger_node_revision
  before update or delete
  on public.nodes
  for each row
  execute procedure trigger_on_node_revision();

/**
 * Threads store thread-level states and are linked to workspaces. Thread ID is
 * a source-node ID.
 */
create table public.threads (
    id bigint not null primary key references public.nodes (id) deferrable initially deferred,
    -- team the thread belongs to. Same as source node's team ID
    workspace_id bigint not null references public.workspaces (id),
     -- if null, thread is not forked
    forked_from_node bigint references public.nodes (id) deferrable initially deferred,
    merge_node bigint references public.nodes (id) deferrable initially deferred,
    is_open boolean default true, -- thread is open to new comments
    attrs jsonb -- thread-level attributes
    is_archived bool not null default false -- soft-delete thread
);

-- Index workspaces to pull threads quicker
create index thread_workspace_idx on threads using btree (workspace_id);

-- gin indexing for thread attributes
create index thread_attrs_idx on threads using gin (attrs jsonb_path_ops);
